# data
data:
  instruct_data: "./datasets/finetune.jsonl"
  eval_instruct_data: "./datasets/verify.jsonl"
  data: ""  # Optionally fill with pretraining data 

# model
model_id_or_path: "~/models/mistral/7B"
lora:
  rank: 64

# optim
seq_len: 32768
batch_size: 1
max_steps: 300
optim:
  lr: 6.e-5
  weight_decay: 0.1
  pct_start: 0.05

# other
seed: 0
log_freq: 1
eval_freq: 100
no_eval: False
ckpt_freq: 100

save_adapters: True  # save only trained LoRA adapters. Set to `False` to merge LoRA adapter into the base model and save full fine-tuned model

run_dir: "run_dir"

wandb:
  project: test-train-mistral-7B
  run_name: test1
  key: 6d55bc3627de1a3a5581fbcc9b9638545831dc02
  offline: False
